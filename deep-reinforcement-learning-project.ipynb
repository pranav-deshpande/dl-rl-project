{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a48dd48a",
   "metadata": {},
   "source": [
    "## Playing Side Scrolling Video Games with Deep Reinforcement Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33f44b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install opencv-python pyautogui pygetwindow \n",
    "# !pip install opencv-contrib-python --user\n",
    "# !pip install pydirectinput \n",
    "# !pip install mss \n",
    "# !pip install gymnasium[all]\n",
    "# !pip install swig \n",
    "# !pip install stable-baselines3[extra]\n",
    "# !pip install pytesseract\n",
    "# !pip install sb3-contrib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5db006e4-7443-4415-b0f2-d673afae3565",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cv2\n",
    "import pytesseract\n",
    "import mss\n",
    "import mss.tools\n",
    "import pygetwindow as gw\n",
    "import time\n",
    "import pyautogui\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "from gymnasium import spaces\n",
    "import sys\n",
    "import os\n",
    "from PIL import Image\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "import pydirectinput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "171313f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the action space\n",
    "jump = 'space'\n",
    "melee_attack = 'ctrl'\n",
    "projectile_attack = 'alt'\n",
    "up = 'up'\n",
    "down = 'down'\n",
    "right = 'right'\n",
    "left = 'left'\n",
    "pick_up = 'z'\n",
    "actions = [up, down, right, left, jump, 'jump_and_climb', 'right_jump', 'left_jump', melee_attack]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "24329880-eb9d-47ad-9a5d-6498b48a8ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_window():\n",
    "    window_title = \"Claw\"\n",
    "    window = gw.getWindowsWithTitle(window_title)[0]\n",
    "    return window\n",
    "\n",
    "def activate_window():\n",
    "    window = get_window()\n",
    "    try:\n",
    "        window.activate()\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "def reset():\n",
    "    activate_window()\n",
    "    pyautogui.press('esc')\n",
    "    pyautogui.press('down')\n",
    "    pyautogui.press('down')\n",
    "    pyautogui.press('down')\n",
    "    pyautogui.press('down')\n",
    "    pyautogui.press('return')\n",
    "    pyautogui.press('return')\n",
    "    time.sleep(2)\n",
    "    pyautogui.press('return')\n",
    "    pyautogui.press('return')\n",
    "    pyautogui.press('return')\n",
    "    time.sleep(2)\n",
    "\n",
    "def close():\n",
    "    window = get_window()\n",
    "    window.close()\n",
    "\n",
    "def start():\n",
    "    claw_path = \"<input_claw_executable_path_here>\"\n",
    "    os.startfile(claw_path)\n",
    "    time.sleep(2)\n",
    "    pyautogui.hotkey('alt', 'return')\n",
    "    activate_window()\n",
    "    time.sleep(15)\n",
    "    activate_window()\n",
    "    time.sleep(2)\n",
    "    activate_window()\n",
    "    pyautogui.press('return')\n",
    "    pyautogui.press('return')\n",
    "    pyautogui.press('return')\n",
    "    time.sleep(3)\n",
    "    activate_window()\n",
    "\n",
    "\n",
    "# Testing these functions\n",
    "# close()\n",
    "# start()\n",
    "# reset()\n",
    "# reset()\n",
    "# They have been perfected!\n",
    "\n",
    "# Function to capture a screenshot of the specified window\n",
    "def capture_screenshot():\n",
    "    # FPS setting\n",
    "    fps = 10\n",
    "    # Interval between captures in seconds\n",
    "    capture_interval = 1.0 / float(fps)  \n",
    "    sct = mss.mss()\n",
    "    # Offset configuration\n",
    "    offsets = {\n",
    "        'left': 8+2,  # Adjust as needed\n",
    "        'top': 32,   # Adjust as needed\n",
    "        'right': 8+2, # Adjust as needed\n",
    "        'bottom': 8+2 # Adjust as needed\n",
    "    }\n",
    "    # Get the window\n",
    "    window = get_window()\n",
    "    bbox = (\n",
    "        window.left + offsets['left'], \n",
    "        window.top + offsets['top'], \n",
    "        window.left + window.width - offsets['right'], \n",
    "        window.top + window.height - offsets['bottom']\n",
    "    )\n",
    "    screenshot = sct.grab(bbox)\n",
    "    return screenshot\n",
    "\n",
    "# Receives numpy image and does lots of pre-processing to obtain the treasure, health and lives\n",
    "def extract_variables(image):\n",
    "    # Set the tesseract cmd\n",
    "    pytesseract.pytesseract.tesseract_cmd = \"<input_tesseract_executable_path_here>\"\n",
    "    # Convert the PIL image to an OpenCV image\n",
    "    # PIL uses RGB and OpenCV uses BGR, so we need to convert the channels\n",
    "    image_cv = cv2.cvtColor(np.array(image), cv2.COLOR_RGB2BGR)\n",
    "    # Assuming the HUD occupies the top 10% of the height of the image\n",
    "    hud_height_ratio = 0.07\n",
    "    hud_height = int(image.height * hud_height_ratio)\n",
    "    # Crop the HUD area from the image\n",
    "    hud_area = image_cv[0:hud_height, :]\n",
    "\n",
    "    treasure = hud_area[:, int(image.width*0.05):int(image.width*.2)]\n",
    "    gray_treasure = cv2.cvtColor(treasure, cv2.COLOR_BGR2GRAY)\n",
    "    blur_treasure = cv2.GaussianBlur(gray_treasure, (3,3), 0)\n",
    "    thresh_treasure = cv2.threshold(blur_treasure, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)[1]\n",
    "    treasure = pytesseract.image_to_string(thresh_treasure, lang='eng', config='--psm 13 --oem 3 -c tessedit_char_whitelist=0123456789')\n",
    "    try:\n",
    "        treasure = int(treasure)\n",
    "    except Exception as err:\n",
    "        treasure = -1\n",
    "    \n",
    "    health = hud_area[:,  int(image.width*0.915):int(image.width*.96)]\n",
    "    gray_health = cv2.cvtColor(health, cv2.COLOR_BGR2GRAY)\n",
    "    blur_health = cv2.GaussianBlur(gray_health, (3,3), 0)\n",
    "    thresh_health = cv2.threshold(blur_health, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)[1]\n",
    "    health = pytesseract.image_to_string(thresh_health, lang='eng', config='--psm 13 --oem 3 -c tessedit_char_whitelist=0123456789')\n",
    "    try:\n",
    "        health = int(health)\n",
    "    except Exception as err:\n",
    "        health = -1\n",
    "    \n",
    "    lives = image_cv[int(0.13*image.height):int(0.165*image.height), int(image.width*0.935):int(image.width*.955)]\n",
    "    gray_lives = cv2.cvtColor(lives, cv2.COLOR_BGR2GRAY)\n",
    "    blur_lives = cv2.GaussianBlur(gray_lives, (3,3), 0)\n",
    "    thresh_lives = cv2.threshold(blur_lives, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)[1]\n",
    "    lives = pytesseract.image_to_string(thresh_lives, lang='eng', config='--psm 13 --oem 3 -c tessedit_char_whitelist=0123456789')\n",
    "    try:\n",
    "        lives = int(lives)\n",
    "    except Exception as err:\n",
    "        lives = -1\n",
    "\n",
    "    return treasure, health, lives\n",
    "\n",
    "def get_frame_and_reward():\n",
    "    screenshot = capture_screenshot()\n",
    "    pil_image = Image.frombytes('RGB', (screenshot.width, screenshot.height), screenshot.rgb)\n",
    "    treasure, health, lives = extract_variables(pil_image)\n",
    "    gray_image = pil_image.convert('L')\n",
    "    resized_pil_img = gray_image.resize((256, 256))\n",
    "    numpy_image = np.array(resized_pil_img).reshape((1, 256, 256))\n",
    "    return numpy_image, treasure #+ health + lives\n",
    "\n",
    "# All the above functions are working, now it is time to define the claw environment\n",
    "\n",
    "def take_action(action):\n",
    "    # print(action)\n",
    "    activate_window()\n",
    "    if action in ['jump_and_climb', 'right_jump', 'left_jump']:\n",
    "        if action == 'jump_and_climb':\n",
    "            pyautogui.hotkey('space', 'up', interval=1./5.)\n",
    "        elif action == 'right_jump':\n",
    "            with pyautogui.hold('space'):\n",
    "                pyautogui.keyDown('right')\n",
    "                time.sleep(1./2.)\n",
    "                pyautogui.keyUp('right')\n",
    "        elif action == 'left_jump':\n",
    "            with pyautogui.hold('space'):\n",
    "                pyautogui.keyDown('left')\n",
    "                time.sleep(1./2.)\n",
    "                pyautogui.keyUp('left')\n",
    "    else:\n",
    "        pydirectinput.keyDown(action)\n",
    "        time.sleep(1./10.)\n",
    "        pydirectinput.keyUp(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fd19d47d-ce9a-4d4e-a37e-a2ee25876d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClawEnv(gym.Env):\n",
    "    def __init__(self):\n",
    "        super(ClawEnv, self).__init__()\n",
    "        self.action_space = spaces.Discrete(len(actions))\n",
    "        self.observation_space = spaces.Box(low=0, high=255, shape=(1, 256, 256), dtype=np.uint8)\n",
    "        try:\n",
    "            close()\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "        start()\n",
    "        \n",
    "    def reset(self, seed=0):\n",
    "        reset()\n",
    "        frame, reward = get_frame_and_reward()\n",
    "        info = dict()\n",
    "         # Stack the initial observation four times\n",
    "        return frame, info\n",
    "\n",
    "    def step(self, action):\n",
    "        take_action(actions[action])\n",
    "        frame, reward = get_frame_and_reward()\n",
    "        # done = 0\n",
    "        terminated = False\n",
    "        truncated = False\n",
    "        info = dict()\n",
    "        if reward % 6 == 0 or reward % 50000000 == 0 or reward == 1 or '6' in str(reward) or '8' in str(reward):\n",
    "            reward = 0\n",
    "        return frame, reward, terminated, truncated, info\n",
    "\n",
    "    def render(self):\n",
    "        pass\n",
    "        \n",
    "    def close(self):\n",
    "        close()\n",
    "\n",
    "# Testing the environment\n",
    "# Instantiate the env\n",
    "env = ClawEnv()\n",
    "# Check if the environment is valid\n",
    "# check_env(env)\n",
    "# Close the env\n",
    "# env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f3503e91-7366-4fbc-aa4f-ee7f279ad9e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    }
   ],
   "source": [
    "from stable_baselines3 import A2C\n",
    "a2c = A2C('CnnPolicy', env, verbose=1).learn(total_timesteps=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "60db39fd-cf94-4a17-88c7-1f7d4c499dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sb3_contrib import RecurrentPPO\n",
    "# ppo = RecurrentPPO('CnnLstmPolicy', env).learn(total_timesteps=25000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "90e29db3-165f-4afe-a32a-2869e7699a7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CnnLstmDQN(\n",
      "  (conv1): Conv2d(1, 32, kernel_size=(8, 8), stride=(4, 4))\n",
      "  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2))\n",
      "  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (pool3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (lstm): LSTM(256, 512)\n",
      "  (head): Linear(in_features=512, out_features=9, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CnnLstmDQN(nn.Module):\n",
    "    def __init__(self, hidden_dim, output_dim):\n",
    "        super(CnnLstmDQN, self).__init__()\n",
    "        \n",
    "        # CNN Feature Extractor\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=8, stride=4)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1)\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        # Computing the output size of the CNN\n",
    "        self.feature_size = self.get_conv_output_size((1, 256, 256))\n",
    "\n",
    "        # LSTM Backbone\n",
    "        self.lstm = nn.LSTM(self.feature_size, hidden_dim)\n",
    "\n",
    "        # Output layer\n",
    "        self.head = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def convolution_operation(self, x):\n",
    "        x = x.squeeze(0)\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool1(x)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool2(x)\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = self.pool3(x)\n",
    "        return x.view(x.size(0), -1)\n",
    "\n",
    "    def get_conv_output_size(self, shape):\n",
    "        with torch.no_grad():\n",
    "            input = torch.rand(1, *shape)\n",
    "            output = self.convolution_operation(input)\n",
    "            return int(torch.numel(output))\n",
    "    \n",
    "    def forward(self, x, hidden=None):\n",
    "        # Forward pass through CNN\n",
    "        x = self.convolution_operation(x)     \n",
    "        # Reshape x for LSTM\n",
    "        x = x.view(1, -1, self.feature_size)\n",
    "        # Forward pass through LSTM\n",
    "        if hidden is None:\n",
    "            x, hidden = self.lstm(x)\n",
    "        else:\n",
    "            x, hidden = self.lstm(x, hidden)\n",
    "        # Output layer\n",
    "        x = self.head(x.squeeze(0))\n",
    "        return x\n",
    "\n",
    "# Example initialization\n",
    "hidden_dim = 512  # Hidden dimensions for LSTM\n",
    "output_dim = len(actions)  # Output dimensions\n",
    "\n",
    "model = CnnLstmDQN(hidden_dim, output_dim)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d09f24-c23a-4801-94be-2cef4986d01b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0, Total Loss: 100000000376832.06\n",
      "Episode 1, Total Loss: 100000000376833.45\n",
      "Episode 2, Total Loss: 3.068405721336603\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "gamma = 0.8\n",
    "env.reset()\n",
    "\n",
    "# Model, loss function, optimizer\n",
    "model = CnnLstmDQN(hidden_dim=512, output_dim=len(actions))\n",
    "criterion = nn.MSELoss()  # Mean Squared Error Loss for DQN\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training parameters\n",
    "num_episodes = 1000  # Number of episodes for training\n",
    "max_steps_per_episode = 25  # Maximum steps in each episode\n",
    "\n",
    "# Loop over episodes\n",
    "for episode in range(num_episodes):\n",
    "    # Reset environment and state at the beginning of each episode\n",
    "    state, _ = env.reset()\n",
    "    total_loss = 0\n",
    "\n",
    "    # Loop over steps within the episode\n",
    "    for step in range(max_steps_per_episode):\n",
    "        # Get action from the policy (model)\n",
    "        # For simplicity, assuming a single-channel grayscale image state\n",
    "        state_tensor = torch.from_numpy(state).unsqueeze(0).unsqueeze(0).float()\n",
    "        action_values = model(state_tensor)\n",
    "        action = action_values.max(1)[1].view(1, 1).item()\n",
    "\n",
    "        # Take action and observe next state and reward\n",
    "        next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "\n",
    "        done = terminated or truncated\n",
    "        \n",
    "        # Preprocess next state\n",
    "        next_state_tensor = torch.from_numpy(next_state).unsqueeze(0).unsqueeze(0).float()\n",
    "\n",
    "        # Compute target Q value\n",
    "        next_action_values = model(next_state_tensor)\n",
    "        max_next_q_value = next_action_values.max(1)[0]\n",
    "        target_q_value = reward + (gamma * max_next_q_value * (1 - done))\n",
    "\n",
    "        # Compute current Q value\n",
    "        current_q_value = action_values.gather(1, torch.tensor([[action]]))\n",
    "\n",
    "        # Compute loss and backpropagate\n",
    "        loss = criterion(current_q_value, target_q_value.unsqueeze(1))\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        state = next_state\n",
    "\n",
    "        # Break if the state is terminal\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    print(f\"Episode {episode}, Total Loss: {total_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "70f3b8e6-2a52-473e-b827-12791f09b695",
   "metadata": {},
   "outputs": [],
   "source": [
    "# At first the the training is unstable but it stabilizes after a few hundred episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "60cf6c59-5860-48ca-9ffb-d46cf3faaf09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gameplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6611429f-5f7d-4aac-a695-34a9f240c92d",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs, _ = env.reset()\n",
    "# Choose model\n",
    "# Custom\n",
    "# play_model = model\n",
    "# Baseline\n",
    "play_model = a2c\n",
    "\n",
    "for i in range(25):\n",
    "    action, _ = a2c.predict(obs)\n",
    "    action = int(action)\n",
    "    obs, reward, terminated, truncated, info = env.step(action)\n",
    "    # print reward)\n",
    "    if terminated or truncated:\n",
    "        env.reset()\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "97e4fdd5-74ed-4504-aa06-fa4fa6911c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to test the custom model, make sure that the output is correctly interpreted after passing the frame throught the model\n",
    "# action_values = model(state_tensor)\n",
    "# action = action_values.max(1)[1].view(1, 1).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d09b72-26ad-4e3d-8986-cc7ab1697031",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
